# ğŸ“„ AI-Powered Interview Question Generator with Automated Evaluation

## ğŸš€ Overview  
The **AI-Powered Interview Question Generator** is a **Streamlit-based web application** that extracts content from **PDF documents**, generates **relevant interview questions** using **LangChain and  Mistral LLM (via Bedrock)**, and evaluates **user responses** using AI. The generated questions are stored in **AWS S3**, making them accessible for further analysis. It leverages **LangChainâ€™s Summarization (Refine Chain)** to process large text inputs efficiently before generating questions. 

This project is designed to help individuals practice interview questions by providing **AI-generated questions** and **detailed feedback on user responses** based on correctness, key points, and improvement suggestions.  

---
# ğŸ¯ Objective  
This project aims to **automate interview question generation** from **PDF documents** using **AI**. It leverages **LangChainâ€™s Refine Chain** for summarization and **Mistral LLM** for question generation. The system evaluates user responses and provides **AI-driven feedback**, making it a valuable tool for:  
- **Interview Preparation** ğŸ“–  
- **Educational Assessments** ğŸ“  
- **Corporate Training** ğŸ¢  

# ğŸš€ Usefulness  
This application is beneficial for:  
âœ… **Students & Job Seekers** â€“ Helps in **preparing for technical interviews** with AI-generated questions.  
âœ… **Educators & Trainers** â€“ Enables **automated quiz generation** from course materials.  
âœ… **HR & Recruiters** â€“ Assists in **candidate screening** by evaluating answers.  
âœ… **Corporate Learning Teams** â€“ Facilitates **interactive training programs** with AI-based assessments.  
âœ… **Content Creators** â€“ Converts PDFs into **structured Q&A formats** for learning platforms.  

## ğŸ¯ Key Features  
- âœ… **PDF Parsing & Text Extraction:** Reads and processes PDFs to extract useful content.
- âœ… **Summarization with LangChain (Refine Chain)** to condense text  
- âœ… **AI-Based Question Generation:** Uses **LangChain & Bedrock (Mistral LLM)** to create coding and conceptual questions.  
- âœ… **Interactive User Input:** Users can answer the generated questions directly within the app.  
- âœ… **Automated AI Evaluation:** The system checks the correctness of answers, identifies missing key points, and suggests improvements.  
- âœ… **Data Storage in AWS S3:** Saves generated questions for future reference and retrieval.  
- âœ… **User-Friendly UI:** Developed using **Streamlit**, making it accessible and easy to use.  

---

## ğŸ›  Tech Stack  

### ğŸ“Œ Programming Languages & Frameworks  
- **Python** â€“ Core programming language  
- **Streamlit** â€“ UI development  
- **LangChain** â€“ LLM orchestration  

### ğŸ¤– AI Models & Processing  
- **Mistral (via bedrock)** â€“ LLM for question generation and evaluation  
- **Amazon Bedrock** â€“ Text embedding model for vector storage  

### â˜ Database & Cloud Storage  
- **ChromaDB** â€“ Vector database for storing question embeddings  
- **AWS S3** â€“ Cloud storage for saving generated questions

## ğŸ— How RAG is Used in This Project?  
1ï¸âƒ£ **PDF Document Parsing & Chunking** ğŸ“„  
   - The uploaded PDF is processed and split into smaller **contextual chunks** using **LangChainâ€™s RecursiveCharacterTextSplitter**.  

2ï¸âƒ£ **Vector Database for Efficient Retrieval** ğŸ—‚  
   - **ChromaDB** stores embeddings of text chunks.  
   - **Amazon Titan-Embed-Text-v1** generates vector representations for efficient similarity search.  

3ï¸âƒ£ **Retrieval of Relevant Context** ğŸ”  
   - When generating interview questions, the system retrieves **relevant chunks** from the vector database.  
   - This ensures that questions are **contextually accurate and well-formed**.  

4ï¸âƒ£ **LLM-Based Question Generation** ğŸ¤–  
   - The retrieved context is passed to **Mistral LLM** via **LangChainâ€™s document chain**.  
   - It formulates **structured interview questions with answers** based on the retrieved information.  




# ğŸ” How It Works?

## ğŸ“Œ Step 1: Upload a PDF File ğŸ“‚  
- The user uploads a **PDF document** containing reference material.  
- The system extracts text . 

## ğŸ“Œ Step 2: Summarization with LangChain (Refine Chain) ğŸ“‘  
- Large documents are **broken into smaller chunks**.  
- **Refine Chain** summarizes each chunk **iteratively**.  
- The final summary is **more readable and contextually accurate**.  

## ğŸ“Œ Step 3: AI-Based Question Generation ğŸ¤–  
- **Mistral LLM via bedrock** generates interview questions.  
- Questions are formulated based on **key topics from the summary**.  
- Generated questions are stored in **AWS S3** for future retrieval.  

## ğŸ“Œ Step 4: User Provides Answers âœï¸  
- The **Streamlit UI** allows users to **answer the generated questions**.  

## ğŸ“Œ Step 5: AI Evaluation & Feedback âœ…âŒ  
- **User responses are compared against expected answers**.  
- Feedback is provided on:  
  - âœ… **Correctness**  
  - âŒ **Missing Key Points**  
  - ğŸ“ **Suggestions for Improvement**
    
    ## ğŸ“¦ Installation & Setup  

### 1ï¸âƒ£ Clone the Repository  
```bash
git clone https://github.com/judirispah/interview-question-generator.git
cd interview-question-generator

python -m venv venv

pip install -r requirements.txt

#Create a .env file in the project directory and add the following credentials:


AWS_KEY_ID=your_aws_key
AWS_ACCESS_KEY=your_aws_secret
AWS_REGION=us-east-1




